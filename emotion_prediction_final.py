# -*- coding: utf-8 -*-
"""emotion_prediction_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qNNZjwnreSqZNGKvYxn1gybZSvfiLa1Q
"""

import pandas as pd
import numpy as np
import re
import nltk
import joblib
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sentence_transformers import SentenceTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score

# Download required NLTK resources
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Load data
df = pd.read_csv('/track-a.csv')
print("Dataset shape:", df.shape)

# Emotion labels
emotion_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']

# Preprocessing function
stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'[^a-z\s]', '', text)
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

# Apply preprocessing
df['clean_text'] = df['text'].apply(preprocess_text)

# Sentence-BERT embeddings
print("Generating sentence embeddings...")
bert_model = SentenceTransformer('all-MiniLM-L6-v2')
X = bert_model.encode(df['clean_text'].tolist())
print("Embedding shape:", X.shape)

# Target labels
y = df[emotion_cols].values
print("Target shape:", y.shape)

# 5-fold Cross-Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
metrics = {emotion: {'precision': [], 'recall': [], 'f1': [], 'roc_auc': []} for emotion in emotion_cols}

for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
    print(f"\nTraining Fold {fold+1}/5...")
    X_train, X_val = X[train_idx], X[val_idx]
    y_train, y_val = y[train_idx], y[val_idx]

    for i, emotion in enumerate(emotion_cols):
        clf = RandomForestClassifier(random_state=42, class_weight='balanced')
        clf.fit(X_train, y_train[:, i])

        y_pred = clf.predict(X_val)
        y_prob = clf.predict_proba(X_val)[:, 1]

        precision = precision_score(y_val[:, i], y_pred, zero_division=0)
        recall = recall_score(y_val[:, i], y_pred, zero_division=0)
        f1 = f1_score(y_val[:, i], y_pred, zero_division=0)
        try:
            roc_auc = roc_auc_score(y_val[:, i], y_prob)
        except:
            roc_auc = np.nan

        metrics[emotion]['precision'].append(precision)
        metrics[emotion]['recall'].append(recall)
        metrics[emotion]['f1'].append(f1)
        metrics[emotion]['roc_auc'].append(roc_auc)

        print(f"{emotion}: Precision={precision:.3f}, Recall={recall:.3f}, F1={f1:.3f}, ROC AUC={roc_auc:.3f}")

# Average metrics
print("\nAverage validation metrics:")
for emotion in emotion_cols:
    print(f"\n{emotion}:")
    for metric in ['precision', 'recall', 'f1', 'roc_auc']:
        scores = [v for v in metrics[emotion][metric] if not np.isnan(v)]
        avg = np.mean(scores) if scores else 0
        print(f"  {metric}: {avg:.3f}")

# Train final models on full data
print("\nTraining final models on full dataset...")
final_models = []
for i, emotion in enumerate(emotion_cols):
    clf = RandomForestClassifier(random_state=42, class_weight='balanced')
    clf.fit(X, y[:, i])
    final_models.append(clf)

# Save models and embedding pipeline
for i, emotion in enumerate(emotion_cols):
    joblib.dump(final_models[i], f'/rf_model_{emotion}.joblib')

bert_model.save('bert_embedding_model')  # Optional

# Load models and embedder
def load_models_and_embedder():
    embedder = SentenceTransformer('all-MiniLM-L6-v2')
    models = []
    base_path = "/content"
    for emotion in ['anger', 'fear', 'joy', 'sadness', 'surprise']:
        model = joblib.load(f'{base_path}/rf_model_{emotion}.joblib')
        models.append(model)
    return embedder, models

# Predict emotions for a single text
def predict_emotions(text):
    clean_text = preprocess_text(text)
    embedder, models = load_models_and_embedder()
    X = embedder.encode([clean_text])  # shape: (1, 384)

    predicted = []
    thresholds = [0.1, 0.3, 0.2, 0.2, 0.25]  # anger, fear, joy, sadness, surprise
    for j, model in enumerate(models):
        prob = model.predict_proba([X[0]])[0][1]
        print(f"{emotion_cols[j]}: {prob:.3f}")
        predicted.append(int(prob > thresholds[j]))

    detected_emotions = [emotion for emotion, val in zip(emotion_cols, predicted) if val == 1]
    return detected_emotions if detected_emotions else ['No emotion detected']

# Threshold-based batch prediction
models = []
for emotion in emotion_cols:
    model = joblib.load(f'/rf_model_{emotion}.joblib')
    models.append(model)

bert_model = SentenceTransformer('all-MiniLM-L6-v2')
thresholds = [0.15, 0.4, 0.3, 0.4, 0.4] # anger, fear, joy, sadness, surprise

def predict_from_text(texts):
    df = pd.DataFrame({'text': texts})
    df['clean_text'] = df['text'].apply(preprocess_text)
    X = bert_model.encode(df['clean_text'].tolist())

    predictions = []
    for i in range(len(X)):
        sample_pred = []
        for j, model in enumerate(models):
            prob = model.predict_proba([X[i]])[0][1]
            sample_pred.append(int(prob > thresholds[j]))
        predictions.append(sample_pred)
    return np.array(predictions)

if __name__ == "__main__":
    while True:
        user_input = input("\nEnter a sentence (or type 'exit' to quit): ")
        if user_input.lower() == 'exit':
            break

        clean_text = preprocess_text(user_input)
        X = bert_model.encode([clean_text])

        for i, model in enumerate(models):
            prob = model.predict_proba([X[0]])[0][1]
            print(f"{emotion_cols[i]} probability: {prob:.3f} (threshold: {thresholds[i]})")

        result = [int(model.predict_proba([X[0]])[0][1] > thresholds[i]) for i, model in enumerate(models)]

        for emotion, value in zip(emotion_cols, result):
            print(f"{emotion}: {value}")